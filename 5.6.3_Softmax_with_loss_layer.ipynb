{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ソフトマックス関数について\n",
    "\n",
    "## 手書き数字認識の場合のSoftmaxレイヤの出力\n",
    "![softmax_function](https://files.slack.com/files-tmb/T0FNB0BP1-F4LMCKYPP-07885f2b72/____________________________2017-03-20_15.39.34_1024.png)\n",
    "\n",
    "Softmaxレイヤは入力された値を正規化（出力のわが1になるように変形）して出力します。\n",
    "\n",
    "\n",
    "ニューラルネットワークで行う処理には、推論(inference)と学習のフェーズがある。\n",
    "推論では、通常Softmaxレイヤは使用しない。最後のAffineレイヤの出力（スコア）を認識結果として用いる。\n",
    "推論で答えと一つだけ出す場合は、スコア裁断値だけに興味があるため、Softmaxレイヤは必要ない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax with Lossレイヤの実装\n",
    "Softmaxと損失関数である交互エントロピー誤差（cross entropy error)も含めたレイヤ\n",
    "![Softmax-with-loss-layer](https://files.slack.com/files-pri/T0FNB0BP1-F4LJWSWCS/____________________________2017-03-20_16.05.11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算グラフではソフトマックス関数はSoftmaxレイヤとして、交差エントロピー誤差はCross Extropy Errorレイヤとして表記する。\n",
    "\n",
    "1. Softmaxレイヤは入力である$(a_1, a_2, a_3)$を正規化して、$(y_1, y_2, y_3)$を出力する。\n",
    "2. Cross Entropy Errorレイヤは Softmaxの出力$(y_1, y_2, y_3)$と教師ラベルの$(t_1, t_2, t_3)$を受取、それらのデータから損失 Lを出力する\n",
    "\n",
    "\n",
    "## 逆伝播の結果について\n",
    "Softmaxレイヤからの逆伝播$(y_1 - t_1, y_2 - t_2, y_3 - t_3 )$ はSoftmaxレイヤの出力と教師ラベルの差分になる。\n",
    "<strong>ニューラルネットワークの逆伝播では、この差分である誤差が前レイヤへ伝わっていく。</strong>\n",
    "\n",
    "ニューラルネットワークの目的は、ニューラルネットワークの出力（Softmaxの出力）を教師ラベルに近づけるように、重みパラメータを調整すること。\n",
    "そのため、ニューラルネットワークの出力と教師ラベルとの誤差を効率よく前レイヤに伝える必要がある。\n",
    "Softmaxレイヤからの逆伝播はまさに、出力と講師ラベルの差である。\n",
    "\n",
    "交差エントロピー誤差という関数は、ソフトマックス関数用の損失関数として設計されたものなので、逆伝播の値がキレイな教師ラベルとの誤差となる。\n",
    "回帰問題では、出力層に「恒等関数」を用い、損失関数として「2乗和誤差」を用いるのも同様の理由。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 例1\n",
    "- 教師ラベルが(0,1,0)\n",
    "- SoftMaxレイヤの出力が(0.3, 0.2, 0.5)\n",
    "-  正解ラベルに対する確率は0.2 (20% = 1 × 0.2)\n",
    "\n",
    "この時点のニューラルネットワークは正しい認識ができていない。\n",
    "Softmaxレイヤからの逆伝播は、(0.3, -0.8, 0.5)という大きな誤差を伝播することになる。\n",
    "\n",
    "#### 例2\n",
    "- 教師ラベルが(0, 1, 0)\n",
    "- Softmaxレイヤの出力が(0.01, 0.99, 0)\n",
    "\n",
    "このニューラルネットワークはかなり正確に認識している。\n",
    "この場合のSoftmaxレイヤからの逆伝播は(0.01, -0.01, 0)と小さな誤差になるた、えSoftmaxレイヤより前にあるレイヤが学習する内容も小さくなる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 損失\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ(one-hot vector)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(y)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size  = self.t.shape[0]\n",
    "        # 伝播する値をバッチの個数（batch_size）で割ることで、データ1個あたりの誤差が前レイヤへ伝播する\n",
    "        dx = (self.y - self,t) / batch_size \n",
    "\n",
    "        return dx\n",
    "\n",
    "    def softmax(a):\n",
    "        c = np.max(a)\n",
    "        exp_a = np.exp(a - c) # オーバーフロー対策\n",
    "        sum_exp_a = np.sum(exp_a)\n",
    "        y = exp_a / sum_exp_a\n",
    "\n",
    "        return y\n",
    "\n",
    "    def cross_entropy_error(y, t):\n",
    "        delta = 1e-7\n",
    "        return -np.sum(t * np.log(y + delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
